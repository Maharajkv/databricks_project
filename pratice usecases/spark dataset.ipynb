{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5b2e94b-57b0-4244-9199-1d462c50c10d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS telecom_catalog_assign\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf22885f-b0ec-40aa-b989-bdb148895f4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create schema and volume\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS telecom_catalog_assign.landing_zone\")\n",
    "spark.sql(\"CREATE VOLUME IF NOT EXISTS telecom_catalog_assign.landing_zone.landing_vol\")\n",
    "\n",
    "# Create required folders in the volume\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c73317d-2410-410d-a182-f0d04b342c29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "customer_csv = '''101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID'''\n",
    "\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0'''\n",
    "\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12'''\n",
    "\n",
    "customer_df = pd.read_csv(StringIO(customer_csv), header=None, names=[\"customer_id\", \"name\", \"age\", \"city\", \"plan\"])\n",
    "usage_df = pd.read_csv(StringIO(usage_tsv), sep=\"\\t\")\n",
    "tower_df = pd.read_csv(StringIO(tower_logs_region1), sep=\"|\")\n",
    "\n",
    "customer_spark_df = spark.createDataFrame(customer_df)\n",
    "usage_spark_df = spark.createDataFrame(usage_df)\n",
    "tower_spark_df = spark.createDataFrame(tower_df)\n",
    "\n",
    "display(customer_spark_df)\n",
    "display(usage_spark_df)\n",
    "display(tower_spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cea498c9-ce5a-4c1d-90ce-48493fa98e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Example DataFrames\n",
    "customer_spark_df = spark.createDataFrame([\n",
    "    Row(customer_id=1, name=\"Alice\"),\n",
    "    Row(customer_id=2, name=\"Bob\")\n",
    "])\n",
    "\n",
    "usage_spark_df = spark.createDataFrame([\n",
    "    Row(customer_id=1, usage=100),\n",
    "    Row(customer_id=2, usage=150)\n",
    "])\n",
    "\n",
    "tower_spark_df = spark.createDataFrame([\n",
    "    Row(tower_id=101, region=\"region1\"),\n",
    "    Row(tower_id=102, region=\"region1\")\n",
    "])\n",
    "\n",
    "# Write DataFrames to the respective volume folders as CSV\n",
    "customer_spark_df.write.mode(\"overwrite\").option(\"header\", True).csv(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\"\n",
    ")\n",
    "usage_spark_df.write.mode(\"overwrite\").option(\"header\", True).csv(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\"\n",
    ")\n",
    "tower_spark_df.write.mode(\"overwrite\").option(\"header\", True).csv(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/\"\n",
    ")\n",
    "\n",
    "# Create empty region2 folder for tower logs\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/\")\n",
    "\n",
    "# Validate files were successfully copied\n",
    "display(dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\"))\n",
    "display(dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\"))\n",
    "display(dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/\"))\n",
    "display(dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee932ede-8b63-4483-ab7f-5e3c372c2145",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Using pathGlobFilter\n",
    "tower_logs_glob_df = spark.read.option(\"header\", True).option(\"pathGlobFilter\", \"*.csv\").csv(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/\"\n",
    ")\n",
    "display(tower_logs_glob_df)\n",
    "\n",
    "# 2. Using list of paths\n",
    "region1_path = \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/\"\n",
    "region2_path = \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/\"\n",
    "tower_logs_multi_df = spark.read.option(\"header\", True).csv([region1_path, region2_path])\n",
    "display(tower_logs_multi_df)\n",
    "\n",
    "# 3. Using recursiveFileLookup\n",
    "tower_logs_recursive_df = spark.read.option(\"header\", True).option(\"recursiveFileLookup\", \"true\").csv(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\"\n",
    ")\n",
    "display(tower_logs_recursive_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bee1bf8-3ed6-4e10-b360-e67aba54a18a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "customer_noheader_noschema_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", False) \\\n",
    "    .option(\"inferSchema\", False) \\\n",
    "    .load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\")\n",
    "display(customer_noheader_noschema_df)\n",
    "\n",
    "customer_header_schema_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\")\n",
    "display(customer_header_schema_df)\n",
    "\n",
    "usage_noheader_noschema_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", False) \\\n",
    "    .option(\"inferSchema\", False) \\\n",
    "    .load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\")\n",
    "display(usage_noheader_noschema_df)\n",
    "\n",
    "usage_header_schema_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\")\n",
    "display(usage_header_schema_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83f32d39-4008-4508-af28-ba7938bfd002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(customer_spark_df.columns)\n",
    "customer_columns = customer_spark_df.columns  \n",
    "customer_named_df = customer_spark_df.toDF(*customer_columns)\n",
    "display(customer_named_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed8edef0-fb1c-45dd-9b1e-c482da8781e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Apply column names using string with toDF for customer data\n",
    "if len(customer_spark_df.columns) == 5:\n",
    "    customer_named_df = customer_spark_df.toDF(\n",
    "        \"customer_id\",\n",
    "        \"name\",\n",
    "        \"age\",\n",
    "        \"city\",\n",
    "        \"plan\"\n",
    "    )\n",
    "    display(customer_named_df)\n",
    "else:\n",
    "    display(customer_spark_df)\n",
    "\n",
    "# 2. Apply column names and datatype using schema function for usage data\n",
    "if len(usage_spark_df.columns) == 4:\n",
    "    usage_schema = \"customer_id INT, voice_mins INT, data_mb FLOAT, sms_count INT\"\n",
    "    usage_schema_df = spark.createDataFrame(\n",
    "        usage_spark_df,\n",
    "        schema=usage_schema\n",
    "    )\n",
    "    display(usage_schema_df)\n",
    "else:\n",
    "    display(usage_spark_df)\n",
    "\n",
    "# 3. Apply column names and datatype using StructType for towers data\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, TimestampType\n",
    "\n",
    "tower_schema = StructType([\n",
    "    StructField(\"event_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"tower_id\", StringType(), True),\n",
    "    StructField(\"signal_strength\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True)\n",
    "])\n",
    "if len(tower_spark_df.columns) == 5:\n",
    "    tower_struct_df = spark.createDataFrame(\n",
    "        tower_spark_df,\n",
    "        schema=tower_schema\n",
    "    )\n",
    "    display(tower_struct_df)\n",
    "else:\n",
    "    display(tower_spark_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark dataset",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
